# Data Pipelines with Apache Airflow

## 1. Apache Airflow 살펴보기
> ### 1.1 데이터 파이프라인 소개
```
1.1.1 데이터 파이프라인 그래프
    {a} -> {b} -> {c}
    : Directed Graph
    : Directed Acyclic Graph (DAG)

1.1.2 데이터 파이프라인 그래프 실행
    1. 다음 태스크로 향하기전 이전 태스크가 완료되었는지 확인
    
    2. 이전 태스크가 완료 되었다면 다음 실행 태스크로 대기열에 추가
    
    3. 실행 대기열에 있는 태스크를 수행, 완료 시 완료 표시
    
    4. 그래프의 모든 태스크가 완료 될때 까지 1단계로 돌아감
    
    (
        - 첫번째 노드의 경우 의존성이 없는 태스크
        - 화살표의 방향성 끝점이 존재하지 않아 업스트림(이전 태스크에 대한) 의존성이 없다.
        => "업스트림의 의존성이 없다면 바로 실행 대기열에 추가 가능하다."
    )
    
1.1.3 그래프 파이프라인과 절차적 스크립트 파이프라인 비교
    만약 의존성이 없는 태스크가 두 개 이상 있을 경우에는
    태스크를 병렬로 실행하여 리소스를 더욱 효율적으로 사용 할 수 있다
    ex)
    {a}->{b}->{c}
                 ->{ㄱ}->{ㄴ}->{ㄷ}
    {1}->{2}->{3}

    여기서 {a}와 {1} 이 병렬처리 되고 {c}와 {3}의 완료로 {ㄱ}의 의존성이 해결되면 {ㄱ} 태스크 진행

    - 또한 병렬실행으로 인해 하나의 오류로 인해 모든 process를 다시 시작하지 않아도 된다.

1.1.4 워크플로 매니저를 이용한 파이프라인 실행
```
> ### 1.2 Airflow 소개
```
1.2.1 파이썬 코드로 유연한 파이프라인 정의
    - Airflow는 파이썬 스크립트로 DAG구조를 구성한다.
    - DAG를 구성하는데 많은 유연성을 제공할 수 있다.

1.2.2 파이프라인 스케쥴링 및 실행
    Airflow의 구성요소
        - Airflow 스케쥴러 : DAG 분석 및 현재 시점에서 DAG 스케줄이 지나면 Airflow 워커에 DAG 태스크 예약
            - 1. 사용자가 DAG 워크플로 작성
            - 2. 스케쥴러 DAG 파일 분석 하고 DAG 태스크 의존성 및 주기 확인
            - 3. DAG의 예약 주기가 경과했는지 확인(만약 예약주기가 현재시간 이전이면 실행)
            - 4. 각 태스크에 대한 의존성 확인, 의존성 태스크가 완료되지 않았다면 실행 대기열에 추가
            - 5. 1단계로 돌아가 새로운 루프를 대기
        - Airflow 워커 : 예약된 태스크를 선택하고 실행
        - Airflow 웹 서버 : 스케쥴러에서 분석한 DAG 시각화 및 DAG 실행과 결과를 확인하는 인터페이스 제공

1.2.3 모니터링과 실패 처리
    Graph view를 통한 의존성 파악
    Tree view를 통한 태스크 진행 상황 및 실패 지점 파악

1.2.4 점진적 로딩 및 백필
    - 정의된 특정 시점에 트리거 가능
    - 최정 시점으로 예상되는 다음 스케쥴 주기를 알려줌 => 각각의 주기로 나누고 주기별 DAG실행 가능
    
    - 데이터 파이프라인을 점진적으로 실행 가능
        - 매전 전체 데이터 세트를 처리하지 않고, 해당 시간슬롯에 대한 데이터만 처리가능

    - 스케쥴 주기와 백필 개념을 결합하면 많은 시간과 비용을 절감하여 작업을 진행할 수 있다.

```

> ### 1.3 언제 Airflow를 사용해야 할까
```
1.3.1 Airflow 선택이유
    Airflow는 배치 지향 데이터 파이프라인 구현하는데 적합
    1. Python 코드를 이용 -> 복잡한 커스텀 파이프라인 생성 가능
    2. Python 기반, 쉽게 확장 가능하고 다양한 시스템과 통합 가능
    3. 스케쥴링 기법을 통한 파이프라인 정기적 실행, 점진적 처리 가능 -> 전체 파이프라인 재실행할 필요가 없다
    4. 백필 기능을 통해 과거 데이터를 쉽게 처릭 가능하다.
    5. Airflow 인터페이스를 통해 파이프라인 실행결과를 모니터링할 수 있고, 오류를 디버깅하기 편하다
1.3.2 Airflow가 적합하지 않은 경우
    1. 반복적이거나 배치 태스크에 특화 => 실시간 데이터 처리 및 해당 파이프라인 처리에 적합하지 않다
    2. 추가 및 삭제 태스크가 빈번한 동적 파이프라인의 경우에는 적합하지 않을 수 있다.
    3. 파이썬 언어에 대한 경험이 없으면 적합하지 않다
    4. 파이프라인 규모가 커지면 복잡성이 크게 증가할 수 있기에, Airflow DAG유지 관리에 엄격한 관리가 필요하다.
```
> ### 1.4 이후 내용
```
필요사항
1. 파이썬 언어의 기본 문법
2. 리눅스 터미널에 대한 기본 활용
3. DB에 대한 지식

요약
    - 데이터 파이프라인은 DAG의 태스크와 그에 대한 의존성을 정의한다
        - 의존성 파악을 통해 태스크를 병렬 처리한다
    - Airflow는 특히 배치 지향 파이프라인 구현을 위해 특화되어 있다
    - Airflow는 
        - 웹서버
        - 스케쥴러
        - 워커
        의 세가지 주요 컴포넌트로 구성된다.
```
## 2. Airflow DAG의 구조

> ### 2.1 다양한 소스에서 데이터 수집
```
2.1.1 데이터 탐색
    - web에서 api를 제공할때 이를 받아와서 처리하는 기능을 만들어 보자
ex)
$ curl -L "https://ll.thespacedevs.com/2.0.0/launch/upcoming"
{
    ...
    "results":[
        {
            "id":"asdf",
            "url":"",
            ...
            },
        {
            "id": ...
            ...
            },

    ]
}
```

> ### 2.2 첫 번째 Airflow DAG 작성
